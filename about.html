<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research</title>
  <link rel="stylesheet" href="style.css">
</head>
  <script>
  document.addEventListener("DOMContentLoaded", function () {
    const dropdowns = document.querySelectorAll(".dropdown");

    dropdowns.forEach(dropdown => {
      const header = dropdown.querySelector(".dropdown-header");
      header.addEventListener("click", () => {
        dropdown.classList.toggle("open");
      });
    });
  });
</script>

<body>

  <nav class="topnav">
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="about.html">Research</a></li>
      <li><a href="teaching.html">Teaching</a></li>
      <li><a href="contact.html">Contact</a></li>
    </ul>
  </nav>

  <div class="container">
  <h1>Research</h1>

  <section>
    <ul>
      <li class="dropdown">
  <div class="dropdown-header">
    <strong><a href="documents/Project4Short.pdf">"Central Bank Communication in Time of Crisis: Different Aspects of Linguistic Complexity"</a>, job market paper (<a href="documents/Project4Appendix.pdf">Appendix</a>)</strong>
    <span class="arrow">&#9660;</span>
  </div>
  <div class="dropdown-content">
    <p>
          Central banks' communication strategies can change to fit the need to be more "persuasive" or to retain more "flexibility". For example, they may wish to stimulate the economy with clear indication that interest rates will be kept low for an extended period. At the same time, this can restrict their freedom to make future decisions appropriately, to deal with the unexpected. These trade-offs can vary according to economic conditions, especially during crisis versus normal times. This paper utilizes natural language tools to examine the textual complexity of policy statements from various central banks to derive not only conventional measurements of textual properties such as readability, but also other features, including abstractness, informativeness, and disunity. We find patterns showing that complexity intensifies during extremely low-growth periods or when economic stimulus is needed. Furthermore, the results reveal significant geographic variation, with differences driven more by regional context than by language. There is also evidence that statements targeting households and firms are far from negligible, underscoring the importance of communicating effectively with the general public. By mapping these patterns, the study provides a deeper understanding of how central banks adapt their communication strategy in times of crisis, contributing to the broader investigation of central bank communication and credibility
    </p>
  </div>
</li>
      <br><br> 
      <li><strong>"Deliberation and Policy Outcomes: Evidence from the FOMC"  </strong> , with Francisco Ruge-Murcia and Alessandro Riboni, work in progress </li>
      <br><br> 
      <li><strong>"Strategic Textual Complexity in Federal Reserve Speeches: Evidence from Political and Economic Turbulence"  </strong> , work in progress </li>
      <br><br>        
<li class="dropdown">
  <div class="dropdown-header">
    <strong><a href="documents/3rdYearPaperOutlines.pdf">"Regulation Complexity Measurements: Methods and Patterns across Time and Industry"</a></strong>
    <span class="arrow">&#9660;</span>
  </div>    
  <div class="dropdown-content">
    <p>
          How do measures, especially complexity measures, based directly on regulation texts relate to economic figures? Can they explain things that conventional numerical variables cannot? To answer this question, we first need to examine and present all possible ways of extracting information directly from legal texts, including those that already exist and newly proposed methods. Regarding the complexity aspect, we found that the new and traditional methods can differ drastically over time and across industries. The new method measurements can also potentially explain some variation in economic figures not reflected by the traditional counterparts. Finally, we also present methods for extracting various textual features that are not complexity for a broader view of legal text patterns.
      </p>
  </div>
      <br><br> 
<li class="dropdown">
  <div class="dropdown-header">
    <strong><a href="documents/Thesis_44145969_Tran_Hoang_Phuong_Linh.pdf">"Applied Causal Inference using Identification Robust Confidence Sets Under Sparsity" </a></strong>
    <span class="arrow">&#9660;</span>
  </div>
  <div class="dropdown-content">
    <p>
         One of the main issues regarding the validity of a regression is when the independent variable is correlated with the error term, resulting in endogeneity problem which leads to biased estimates.. 2SLS is typically used with several assumptions such as exclusion restrictions and strong instruments. This paper applies a new method proposed in Gautier et al. (2018) (SNIV) with the idea of sparsity, which relaxes the standard 2SLS assumptions on the excluded instruments specication and instrument strength. We rst consider the same exclusion restrictions as 2SLS, followed by relaxing the exact location of the excluded instruments. This is the rst time SNIV has been applied to these real-life datasets, and by comparing the outcomes, the similarities and the discrepancies in the outcomes will accordingly insinuate the possible validation of the assumptions made for 2SLS. The conclusions show, under similar assumptions of exclusion restrictions, 2SLS and SNIV yield similar results when the instruments are strong, while the former estimates and standard errors are misleading when the instruments are weak. Relaxing excluded restrictions also shed lights on the behaviour of SNIV in dierent datasets, including sample size and strength of the instruments.  
    </p>
  </div>
</li>
      
</ul>
  </section>

  <!-- <section>
    <h2>Relevant Experience</h2>
    <ul>
      <li><strong>Research assistance </strong>  <br>
        First project: The determinants of the student experience (especially first-year students), the retention rates and the relationship between these two variables using datasets from UQ students. The goal is to understand and improve the retention rates of university students. My responsibilities have included: (i) Working as a Casual SRN (Student Relations Network) Crew member, and (ii) Data collection, analysis, and literature reviews for conclusions. Under Professor Renuka Mahadeva. <br>
        Second project: The determinants of the digitalization degree of firms and the speed of adapting new technologies to the current business. The goal is to understand which characteristics of the firms are the most important. My responsibilities have included data collection and analysis. Under Professor Renuka Mahadeva</li>
      <li><strong>Other projects </strong> <br>
        <a href="documents/Treasury_essay.pdf">Entry to treasury competition</a>: What do you believe will be most important for ensuring Australiaâ€™s future economic prosperity, and why?<br>
        Analytics Vidhya and Kaggle competition: Black Friday sales prediction, Big Mart sale prediction, HR analytics, Feedback Prize - English Language Learning (Evaluating language knowledge of ELL students from grades 8-12), etc.</li>
    </ul>
  </section> -->

</div>
</body>

</html>




















